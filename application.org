* Current eDNA analysis tools
For metagenomic eDNA data sets there are several different pipelines
available for analysis. The vast majority of approaches has been
developed and focused on analysis of bacterial communities as these
are the most common data sets. This means that most approaches might
have to be adjusted for them to work on other organisms groups, but in
most cases other groups are less diverse and hence easier to
analyse. Depending on assumptions made and the type of data that is
collected, most analysis can be divided into the following step.

1. Filter and de-multiplex raw data.
   This step makes sure that only high quality sequence data is
   retained for the downstream analysis steps. If samples have been
   run with different index tags this steps often sorts the data
   according to these tags.

2. Collapse the collected sequence data into unique entries.
   This step will take the highly redundant raw data and count how
   many times each sequence is found in the data set. Due to PCR
   amplification and sequencing errors as well as DNA variation within
   species this step is not as trivial as it might seem.

3. Assigning the collected sequences to species.  
   By using sequence similarities to either local curated sequence
   data bases or public sequence repositories taxonomy for identified
   sequences can be inferred. In well known more confined groups
   (for freshwater fish species of Scandinavia) this step is most
   easily done by matching collected data with a curated data base of
   know species. In other cases where the organism group is more
   diverse and that might contain previously uncharacterized species
   it is often done by simply comparing to all of the nt database at
   NCBI or 

   
Depending on pipelines used the steps are not always done in the same
order as mentioned above. Many pipelines will for example use
reference databases already in step two. The idea being that a match
to a database can be directly assigned to the taxonomy of this species
leaving room to focus resources on inferring the taxonomy of the novel
sequences with more computationally demanding approaches.

Among the more popular pipelines is the QIIME 2 that is optimized for
microbial data set and contains tools from raw data to taxonomy
assignment. Via open APIs and plugins it offers great flexibility to
extend and adapt the tool to fit for other organism groups. Obitools
is another fairly common tool that is less focused on microbial data
and have many convient tools for looking at DNA
metabarcoding. Especially the tool called ecoPCR have been employed
also outside the pipeline as an efficient tool to reduce the size of
the database that one use as reference.

* Standards in the face of a moving target
Since there are major leaps forward in the field of DNA analysis it is
today hard to see that one can specify anything but very broad
recommendations as the technology is developing at pace where machines
and chemistry that were used 5 years ago are no longer competetive and
might not even be available. In addition, the use of reference
databases for determining species compositition will as soon as new
releases of the databases are done have to re-evaluate the results
from these. For example, if a single spiecies from a genera is part of
a database it is very likely that we will annotate as sequence from
this genera to the species (at least if they are not that distant) and
then when the rest of the species in the same group are added to the
database it might no longer be assigned the this original species, but
instead only to the correct genera. The same reasoning holds for any
taxonomic level. 

As an example, if we look at the number of species in
the largest sequence data base NCBI. Looking at only at the number of
recorded eukaryote species (excluding informal names and unclassified
entries) there were around 42 thousand species by the year 2000 and
~206 thousand in 2010 to 398 thousand in 2018. This means that new
sequences and taxonomic knowledge is still added at a very high
rate. This hence means that pipelines that early on rely on database
compositions should be avoided and the sequence content is the
collected data that is valuable. The transfer to taxonomy etc should
be restrained to the very end of analysis and should preferably upon
new database releases be redone.

In such unstable enviroments we should hence at the moment instead of
making a standard that everyone should follow allow for flexibility in
terms of collected data, but focus efforts on collecting metadata
about experiments and make both raw data, analysis pipelines
(including cut-off and settings) and analysed data available to the
public and easily accessible. 

It hence central that the community strive for finding standardised
sampling and experimental approaches, but the differences among
species groups and the many factors that influence eDNA abundance and
the ability to extract this from samples means that we are not likely
to find a single method that will work under all circumstances. If we
however make sure to collect experimental metadataa and data in a
central open access repoitory we will have the opportunity to not only
make use of data that are collected using different approaches and
with different questions in mind, but more importantly use the
benefits of large data set and learn which factors influence the
outcome of an experiment. By having it open access is also important
to be able to make it easier to create time series data.


They had data from just over
34000 species of eukaryotes by the end of 1999. In 2005 this number
had gone up to more than 195000 species. Moving 5 years forward and
the number of spiecies is ~438 thousand and the coming 5 years the
number of eukaryotes almost doubled to ~804 thousand. And during the
last three years the number of eukaryote species have increasd with on
average more than 100 thousand species per year. There is hence still
lots 
